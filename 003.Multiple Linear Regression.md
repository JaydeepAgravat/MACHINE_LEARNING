# Multiple Linear Regression

Multiple Linear Regression (MLR) is a statistical technique used to model the relationship between a dependent variable (target) and two or more independent variables (predictors or features). It is an extension of simple linear regression, which deals with only one independent variable. MLR assumes that there is a linear relationship between the dependent variable and the independent variables. The goal of MLR is to find the best-fitting linear equation that describes this relationship.

Here's a detailed explanation of Multiple Linear Regression:

1. **Terminology**:
   - Dependent Variable (DV): This is the variable you want to predict or explain. It is denoted as Y.
   - Independent Variables (IVs): These are the variables used to predict the dependent variable. In multiple linear regression, there are two or more independent variables, denoted as X₁, X₂, X₃, etc.
   - Coefficients: These are the parameters of the linear equation that need to be estimated. In MLR, you have a coefficient for each independent variable, denoted as β₁, β₂, β₃, etc. These coefficients represent the effect of each independent variable on the dependent variable while holding other variables constant.
   - Intercept: This is the constant term in the linear equation, denoted as β₀. It represents the predicted value of the dependent variable when all independent variables are set to zero.

2. **Assumptions**:
   - Linearity: MLR assumes that the relationship between the independent variables and the dependent variable is linear.
   - Independence: The observations should be independent of each other.
   - Homoscedasticity: The variance of the errors (residuals) should be constant across all levels of the independent variables.
   - Multicollinearity: Independent variables should not be highly correlated with each other.
   - Normally Distributed Residuals: The residuals should follow a normal distribution.

3. **The Multiple Linear Regression Equation**:
   The multiple linear regression equation can be written as:

   Y = β₀ + β₁X₁ + β₂X₂ + β₃X₃ + ... + βₖXₖ + ε

   - Y is the predicted value of the dependent variable.
   - β₀ is the intercept.
   - β₁, β₂, β₃, ..., βₖ are the coefficients for the independent variables X₁, X₂, X₃, ..., Xₖ.
   - ε represents the error term, which accounts for the variability in Y that is not explained by the independent variables.

4. **Estimation**:
   The coefficients (β₀, β₁, β₂, etc.) are estimated using various techniques such as the method of least squares, which minimizes the sum of squared differences between the actual and predicted values of the dependent variable.

5. **Model Evaluation**:
   Common methods for evaluating the performance of a multiple linear regression model include:
   - R-squared (R²): A measure of the proportion of variance in the dependent variable that is explained by the independent variables.
   - Adjusted R-squared: An adjusted version of R² that takes into account the number of independent variables in the model.
   - Residual analysis: Examining the residuals to check if the assumptions of the model are met.
   - Hypothesis testing: Testing the significance of individual coefficients and overall model significance.

6. **Predictions**:
   Once the model is fitted, you can use it to make predictions. Given values of the independent variables, you can plug them into the regression equation to predict the value of the dependent variable.

Multiple Linear Regression is a powerful tool for modeling complex relationships between multiple variables, making it valuable in fields such as economics, finance, social sciences, and many other domains where understanding and predicting relationships between variables are essential.

## The Multiple Linear Regression Equation

In the context of Multiple Linear Regression (MLR), you can represent the method of least squares (MLS) using matrix notation. This allows you to express the estimation of coefficients in a more compact and general form. Here are the key formulas in matrix notation:

1. **MLR Equation**:
   In matrix notation, the MLR equation can be represented as:

   Y = Xβ + ε

   - Y is the vector of observed values of the dependent variable.
   - X is the matrix of independent variables, where each row represents an observation, and each column represents a different independent variable.
   - β is the vector of coefficients to be estimated.
   - ε is the vector of errors or residuals.

2. **Predicted Values**:
   The predicted values of the dependent variable (Ŷ) are obtained by multiplying the matrix of independent variables (X) by the coefficient vector (β):

   Ŷ = Xβ

3. **Residuals**:
   The residuals (ε) are the vector of differences between the observed values of the dependent variable (Y) and the predicted values (Ŷ):

   ε = Y - Ŷ

4. **Sum of Squared Residuals (SSE)**:
   The SSE, which represents the overall goodness-of-fit of the model, is calculated as the sum of the squared residuals:

   SSE = εᵀε

   Here, εᵀ represents the transpose of the ε vector.

5. **Coefficient Estimates**:
   The goal of MLS in matrix notation is to estimate the coefficient vector (β) that minimizes the SSE. The estimates for β can be obtained using the following formula:

   β-hat = (XᵀX)⁻¹XᵀY

   - Xᵀ represents the transpose of the X matrix.
   - (XᵀX)⁻¹ represents the inverse of the matrix product XᵀX.
   - XᵀY represents the matrix product of the transpose of X and Y.

6. **Intercept Estimate**:
   The estimate for the intercept (β₀) is typically incorporated by adding a column of ones to the independent variable matrix X. Then, the intercept estimate is included as the first element of the coefficient vector β-hat.

By using matrix notation, you can efficiently represent and compute the MLR model's parameters, making it suitable for more complex MLR models with multiple independent variables. This notation also facilitates the use of linear algebra techniques for solving the MLS equations and estimating coefficients.

## Advantages & Disadvantages of MLS

The Method of Least Squares (MLS) in the context of Multiple Linear Regression (MLR) offers several advantages and disadvantages:

**Advantages:**

1. **Simplicity:** MLS is a straightforward and easy-to-understand method for estimating the coefficients in a linear regression model. Its simplicity makes it accessible to a wide range of users.

2. **Optimal Estimates:** MLS provides estimates for the coefficients (β₀, β₁, β₂, ...) that minimize the sum of squared differences between the observed and predicted values of the dependent variable. These estimates are considered optimal in the sense that they maximize the likelihood of the observed data given the model.

3. **Statistical Inference:** MLS provides a solid foundation for performing statistical hypothesis tests on the coefficients, such as testing whether a particular coefficient is significantly different from zero. This allows you to assess the significance of each independent variable in explaining the variation in the dependent variable.

4. **Linear Assumption:** If the relationship between the dependent and independent variables is approximately linear, MLS tends to work well. It's a good choice for modeling relationships when you have a theoretical reason to expect linearity.

5. **Widely Applicable:** MLS can be applied to various regression problems beyond MLR, including simple linear regression, polynomial regression, and more complex forms like logistic regression or multiple logistic regression.

**Disadvantages:**

1. **Assumptions:** MLS relies on several assumptions, including linearity, independence of errors, homoscedasticity (constant variance of errors), and normally distributed residuals. If these assumptions are violated, the model's estimates may be biased or inefficient.

2. **Overfitting:** In MLR, especially when many independent variables are included, there is a risk of overfitting the model to the training data. This can lead to poor generalization to new, unseen data.

3. **Multicollinearity:** When independent variables are highly correlated with each other (multicollinearity), MLS may produce unstable and unreliable coefficient estimates. It becomes challenging to discern the individual impact of each variable on the dependent variable.

4. **Limited to Linear Relationships:** MLS assumes that the relationship between the dependent and independent variables is linear. If the true relationship is nonlinear, MLS may provide poor model fits and inaccurate predictions.

5. **Outliers and Influential Points:** MLS is sensitive to outliers and influential data points. A single outlier can disproportionately affect the coefficients and the model's fit.

6. **Data Availability:** MLS requires a sufficient amount of data to yield reliable estimates, especially when dealing with a large number of independent variables. If you have limited data, the model's performance may be compromised.

7. **Model Complexity:** As the number of independent variables increases in MLR, the model becomes more complex and challenging to interpret. This complexity can lead to difficulties in explaining the relationships between variables.

In summary, the Method of Least Squares is a powerful and widely used method for estimating coefficients in linear regression models. However, its performance depends on the adherence to underlying assumptions and the characteristics of the data. It's important to assess the model's assumptions and consider potential limitations when applying MLS in practice.

## Is sklearn.linear_model.LinearRegression use this β = (XᵀX)⁻¹XᵀY

Scikit-Learn's `linear_model.LinearRegression` module typically does not use the closed-form solution β = (XᵀX)⁻¹XᵀY to calculate the coefficients in linear regression. Instead, it employs a more numerically stable and computationally efficient method based on the Singular Value Decomposition (SVD) of the design matrix X. This approach is preferred in practice, especially when dealing with high-dimensional data or potentially ill-conditioned matrices.

The closed-form solution, while mathematically elegant, can be numerically unstable and prone to issues when dealing with collinear independent variables or near-singular matrices. The SVD-based approach avoids these numerical problems.

Scikit-Learn's `linear_model.LinearRegression` module uses the SVD-based approach to fit the linear regression model. It solves the linear regression problem by finding the Moore-Penrose pseudo-inverse of the design matrix X (denoted as X⁺) and then calculates the coefficients using the formula:

β = X⁺Y

This method provides robust and stable results for linear regression and is more commonly used in practice due to its numerical advantages over the closed-form solution β = (XᵀX)⁻¹XᵀY.

The SVD-based approach has several advantages over the closed-form solution β = (XᵀX)⁻¹XᵀY:

1. Numerical stability: It is less prone to numerical instability, especially when X is ill-conditioned or when there is multicollinearity among the independent variables.

2. Robustness: It can handle scenarios where the design matrix may not have full rank.

3. Efficiency: It is computationally efficient and avoids the need to explicitly compute the matrix inverse.

This approach is widely used in practice for fitting linear regression models, especially when dealing with real-world datasets that may have numerical challenges.

## SVD-based approach to fit the linear regression model

The SVD-based approach to fitting a linear regression model addresses several problems that the closed-form solution β = (XᵀX)⁻¹XᵀY can encounter, especially when dealing with certain data characteristics. Here are the key problems solved by the SVD-based approach:

1. **Numerical Stability**:
   - **Issue with β = (XᵀX)⁻¹XᵀY**: The closed-form solution involves matrix inversion, which can be numerically unstable when the matrix XᵀX is ill-conditioned or nearly singular. Small numerical errors can lead to unreliable coefficient estimates.
   - **Solved by SVD**: The SVD-based approach avoids direct matrix inversion and performs more stable computations. It is less sensitive to ill-conditioned matrices and is less likely to suffer from numerical instability.

2. **Collinearity and Near-Singularity**:
   - **Issue with β = (XᵀX)⁻¹XᵀY**: When independent variables in X are highly correlated (collinearity) or when X is nearly singular (near-singularity), the closed-form solution can produce unstable or unreliable coefficient estimates.
   - **Solved by SVD**: The SVD decomposition helps identify collinearity and near-singularity in the data. By computing the pseudo-inverse of Σ (Sigma) with care, the SVD-based approach stabilizes coefficient estimation and provides more robust results in the presence of collinear or nearly dependent variables.

3. **Matrix Inversion Requirements**:
   - **Issue with β = (XᵀX)⁻¹XᵀY**: The closed-form solution requires that XᵀX be invertible, which may not always be the case, especially when dealing with high-dimensional data.
   - **Solved by SVD**: The SVD-based approach does not explicitly require matrix inversion, making it more applicable to situations where XᵀX is not invertible. It can handle scenarios where the design matrix does not have full rank.

4. **Computational Efficiency**:
   - **Issue with β = (XᵀX)⁻¹XᵀY**: In some cases, especially with large datasets, the matrix operations involved in the closed-form solution can be computationally expensive.
   - **Solved by SVD**: The SVD-based approach is often more computationally efficient and can be applied to large datasets without excessive computational burden.

5. **Robustness**:
   - **Issue with β = (XᵀX)⁻¹XᵀY**: The closed-form solution may not be robust in the presence of outliers or noisy data.
   - **Solved by SVD**: The SVD-based approach is generally more robust to outliers and noise because it focuses on decomposing the data matrix into singular values and vectors, which helps filter out noise.

In summary, the SVD-based approach offers numerical stability, robustness, and better handling of collinearity and near-singularity issues compared to the closed-form solution. It is a more reliable and versatile method for estimating coefficients in linear regression, particularly in situations where the closed-form solution may encounter problems.

## Derivation of Multiple Linear Regression Coefficient Estimation Formula (Closed-Form Solution)

Certainly! Let's derive the formula for estimating the coefficients β in multiple linear regression using the closed-form solution step by step, writing each equation along the way:

**Step 1: Linear Regression Model**
Start with the multiple linear regression model:

Y = Xβ + ε

- Y is the vector of observed values of the dependent variable.
- X is the matrix of independent variables.
- β is the vector of coefficients to be estimated.
- ε is the vector of errors or residuals.

**Step 2: Residuals**
Express the residuals (ε) as the difference between the observed values (Y) and the predicted values (Ŷ):

ε = Y - Ŷ

**Step 3: Sum of Squared Residuals (SSE)**
The objective in linear regression is to minimize the sum of squared residuals (SSE):

SSE = Σ(εᵢ)² for all observations i from 1 to n

**Step 4: Objective Function**
To minimize SSE, take the derivative of SSE with respect to β and set it equal to zero:

∂SSE/∂β = 0

**Step 5: Express SSE**
SSE can be expressed as:

SSE = εᵀε

Here, εᵀ represents the transpose of the ε vector.

**Step 6: Derivative**
Take the derivative of SSE with respect to β:

∂SSE/∂β = ∂(εᵀε)/∂β

**Step 7: Apply the Chain Rule**
Apply the chain rule to the derivative of εᵀε:

∂(εᵀε)/∂β = ∂(εᵀε)/∂ε * ∂ε/∂β

**Step 8: Calculate ∂(εᵀε)/∂ε**
The derivative of εᵀε with respect to ε is:

∂(εᵀε)/∂ε = 2ε

**Step 9: Calculate ∂ε/∂β**
Calculate the derivative of ε with respect to β. Recall that Ŷ = Xβ:

∂ε/∂β = ∂(Y - Ŷ)/∂β = -X

**Step 10: Combine the Derivative Components**
Combine the derivative components:

∂SSE/∂β = 2εᵀ(-X)

**Step 11: Set Derivative Equal to Zero**
Set the derivative equal to zero to find the coefficients that minimize SSE:

2εᵀ(-X) = 0

**Step 12: Solve for β**
Solve for β:

-2Xᵀε = 0

**Step 13: Isolate β**
To isolate β, divide both sides by -2Xᵀ:

Xᵀε = 0

**Step 14: Substitute ε**
Recall that ε = Y - Ŷ, where Ŷ = Xβ:

Xᵀ(Y - Xβ) = 0

**Step 15: Distribute Xᵀ**
Distribute Xᵀ across the terms:

XᵀY - XᵀXβ = 0

**Step 16: Move XᵀXβ to the Other Side**
Move XᵀXβ to the other side of the equation:

XᵀY = XᵀXβ

**Step 17: Isolate β**
To isolate β, we can multiply both sides by the inverse of XᵀX, denoted as (XᵀX)⁻¹:

(XᵀX)⁻¹XᵀY = β

So, that's how we arrive at the formula for estimating the coefficients β in multiple linear regression:

β = (XᵀX)⁻¹XᵀY

# Regression Analysis

Regression analysis is a statistical method used to examine the relationship between one or more independent variables (predictors or features) and a dependent variable (the outcome or response). It is a fundamental tool in statistics and data analysis, widely applied in various fields, including economics, social sciences, finance, and natural sciences. Regression analysis helps in understanding and quantifying the relationships between variables and can be used for prediction, hypothesis testing, and model building.

Here's a detailed explanation of regression analysis:

1. **Types of Regression Analysis:**
   There are several types of regression analysis, but the most common ones include:

   - **Simple Linear Regression:** Involves one independent variable and one dependent variable. It aims to establish a linear relationship between them. The equation of a simple linear regression model is typically represented as Y = a + bX, where Y is the dependent variable, X is the independent variable, 'a' is the intercept, and 'b' is the slope of the line.

   - **Multiple Linear Regression:** Involves more than one independent variable and one dependent variable. It extends the concept of simple linear regression by fitting a linear equation to multiple predictors. The equation can be written as Y = a + b1X1 + b2X2 + ... + bnXn.

   - **Logistic Regression:** Used when the dependent variable is binary (0/1 or Yes/No). It models the probability of the binary outcome using a logistic function.

   - **Polynomial Regression:** Used when the relationship between the independent and dependent variables is not linear but can be approximated by a polynomial equation.

   - **Ridge, Lasso, and Elastic Net Regression:** These are variants of linear regression used for regularization when dealing with multicollinearity and overfitting issues.

2. **Key Concepts:**

   - **Dependent Variable (Y):** The variable you are trying to predict or explain. It is the outcome variable.

   - **Independent Variable (X):** The variable(s) that are used to predict or explain the dependent variable.

   - **Regression Coefficients (a and b):** These coefficients represent the intercept (a) and the slope (b) of the regression line. They quantify the relationship between the independent and dependent variables.

   - **Residuals:** Residuals are the differences between the observed values (actual values) and the predicted values (values from the regression model). A good regression model should have small residuals.

   - **Goodness of Fit:** Measures like R-squared (R²) or adjusted R-squared indicate how well the model fits the data. A higher R-squared value suggests a better fit.

3. **Steps in Regression Analysis:**

   - **Data Collection:** Gather data for the dependent and independent variables.

   - **Data Preprocessing:** Clean the data by handling missing values, outliers, and transforming variables if necessary.

   - **Model Selection:** Choose the appropriate type of regression based on the nature of your data and research question.

   - **Model Training:** Use a suitable algorithm (e.g., ordinary least squares for linear regression) to estimate the coefficients of the model.

   - **Model Evaluation:** Assess the model's goodness of fit using metrics like R-squared, root mean square error (RMSE), or mean absolute error (MAE).

   - **Inference and Interpretation:** Analyze the coefficients to understand the relationships between variables and make inferences about the data.

   - **Prediction:** If the primary goal is prediction, use the trained model to make predictions on new data.

4. **Assumptions of Regression Analysis:**

   - Linearity: The relationship between independent and dependent variables should be linear.

   - Independence: Observations should be independent of each other.

   - Homoscedasticity: The variance of the residuals should be constant across all values of the independent variable(s).

   - Normally Distributed Residuals: Residuals should follow a normal distribution.

5. **Limitations:**

   - Regression analysis assumes a linear relationship, which may not always hold in real-world scenarios.

   - It can be sensitive to outliers, and outliers can strongly influence the model.

   - It assumes that the model's assumptions are met, which may not always be the case.

   - Correlation does not imply causation; finding a relationship between variables does not mean that one variable causes the other.

Regression analysis is a powerful tool for understanding and predicting relationships between variables, but it should be used carefully, and the assumptions and limitations should be considered when interpreting the results.

Certainly, let's consider a complex example of multiple linear regression in the context of real estate. Imagine you are trying to predict the selling price of houses based on various factors. Your dataset includes the following variables:

1. **Dependent Variable (Y):**
   - Selling Price of the House

2. **Independent Variables (X):**
   - Size of the house (in square feet)
   - Number of bedrooms
   - Number of bathrooms
   - Distance to the nearest school (in miles)
   - Neighborhood crime rate
   - Year of construction
   - Presence of a garage (0 for no, 1 for yes)

Here's a step-by-step breakdown of how you might perform a multiple linear regression analysis for this real estate example:

**1. Data Collection:**
   You collect data on house sales, including the selling prices and the attributes mentioned above for a sample of houses in a particular city.

**2. Data Preprocessing:**

- Clean the data by handling missing values and outliers.
- Transform categorical variables like "Presence of a garage" into binary (0 or 1) using one-hot encoding.
- You might also standardize or normalize the continuous variables for consistency.

**3. Model Selection:**
   You choose multiple linear regression since you have several independent variables and want to estimate how each of them contributes to the selling price.

**4. Model Training:**
   Use a statistical software or programming language (e.g., Python with libraries like scikit-learn or R) to estimate the coefficients of the model. The model might look like:

   Selling Price = a + b1*Size + b2*Bedrooms + b3*Bathrooms + b4*Distance to School + b5*Crime Rate + b6*Year of Construction + b7*Garage Presence

   The coefficients (b1, b2, b3, b4, b5, b6, b7) are estimated during this step.

**5. Model Evaluation:**

- Assess the goodness of fit using metrics like R-squared (R²), mean squared error (MSE), or root mean square error (RMSE).
- A high R-squared value indicates how well the model explains the variance in selling prices.

**6. Inference and Interpretation:**
Analyze the coefficients:

- A positive coefficient (e.g., b1) suggests that an increase in the corresponding variable (Size) leads to an increase in the selling price.
- A negative coefficient (e.g., b4) suggests that an increase in the Distance to School may lead to a decrease in the selling price.
- The magnitude of the coefficients indicates the strength of the effect.

**7. Prediction:**
   Once your model is trained and validated, you can use it to predict the selling price of houses with specific attributes in the future or assess the impact of changes in these attributes.

**8. Assumptions and Limitations:**

- Ensure the assumptions of linear regression, like linearity, independence, homoscedasticity, and normally distributed residuals, are met.
- Remember that correlation does not imply causation. While the model can show relationships, it doesn't prove causation between variables.

This complex example demonstrates how multiple linear regression can be used in a real-world scenario to predict a house's selling price based on a combination of factors, which can be valuable for both buyers and sellers in the real estate market.

## Why ML problems are a Statistical Inference Problems ?

Machine learning (ML) problems can be viewed as statistical inference problems because they both involve drawing conclusions or making predictions based on data. ML models are essentially data-driven statistical models that learn patterns and relationships from data.

Here's a simple example to illustrate this connection:

**Example: Predicting Exam Scores**
Imagine you want to predict students' exam scores based on the number of hours they studied. In this scenario:

- In a statistical inference context, you might use linear regression to estimate the relationship between the number of study hours (independent variable) and exam scores (dependent variable). You would be interested in understanding the slope and intercept of the regression line and testing the significance of this relationship.

- In an ML context, you would use the same data to train a machine learning model, like a linear regression model, to predict exam scores based on study hours. The model learns from the data and makes predictions. Here, you are essentially inferring the relationship between study hours and exam scores based on the learned model.

In both cases, you are using data to make predictions and infer relationships. The key distinction is that in ML, the focus is often on optimizing predictive performance, while in statistical inference, the focus may be on understanding the statistical significance of relationships. However, the fundamental goal of using data to draw conclusions remains the same.

## Inference Vs Prediction & Why regression analysis is required ?

Inference and prediction are two distinct goals in the context of regression analysis, and understanding the difference between them helps clarify why regression analysis is necessary. Let's explore these concepts:

**1. Inference:**

- **Goal:** In statistical inference, the primary goal is to understand and interpret the relationships between variables. This often involves estimating and testing the parameters of a statistical model.
- **Use Case:** Inference is commonly used in scientific research, social sciences, and epidemiology, where researchers want to understand how one or more independent variables are associated with or affect the dependent variable. They may be interested in testing hypotheses, examining causality, and uncovering underlying relationships.
- **Example:** Researchers might use regression analysis to investigate how factors like smoking, diet, and exercise are related to the incidence of a particular health condition.

**2. Prediction:**

- **Goal:** In prediction, the focus is on building models that make accurate forecasts or estimations of future or unseen data points. The emphasis is on model performance and generalization to new data.
- **Use Case:** Prediction is prevalent in fields like machine learning, finance, and business, where the primary objective is to make practical, actionable predictions. This could involve predicting stock prices, customer behavior, or weather conditions.
- **Example:** A retail company might use regression analysis to predict future sales based on historical data to optimize inventory and staffing.

**Why Regression Analysis is Required:**

Regression analysis is a versatile and widely used statistical method that is required for the following reasons:

1. **Modeling Relationships:** Regression analysis allows you to model and quantify the relationships between variables. Whether you're interested in understanding how variables are associated (inference) or making predictions, regression provides a systematic framework to do so.

2. **Interpreting Relationships:** Regression models provide interpretable coefficients that describe the magnitude and direction of the relationship between variables. This is essential for making informed decisions and understanding the implications of the data.

3. **Handling Multiple Variables:** Regression analysis can handle cases where multiple independent variables influence a dependent variable. This is valuable for real-world scenarios where multiple factors are at play.

4. **Flexibility:** Regression comes in various forms, including linear, logistic, polynomial, and more. This flexibility allows you to choose the most suitable model for your specific problem, whether it's for inference or prediction.

5. **Diagnostic Tools:** Regression analysis offers a range of diagnostic tools to assess the validity and reliability of the model, such as checking for multicollinearity, heteroscedasticity, and outliers.

6. **Understanding and Validation:** Even in prediction tasks, regression analysis helps you understand which variables are most important for the prediction and allows you to validate the model's performance.

In summary, regression analysis is required because it provides a framework for both inference and prediction, allowing you to uncover relationships between variables and make practical, data-driven decisions. Depending on your specific goals, you can use regression to either understand the underlying factors (inference) or make accurate predictions (prediction).

## TSS, RSS and ESS

TSS (Total Sum of Squares), RSS (Residual Sum of Squares), and ESS (Explained Sum of Squares) are essential components in the context of regression analysis. They are used to assess the goodness of fit of a regression model and to understand the variance in the data. Here's a detailed explanation of each:

1. **TSS (Total Sum of Squares):**
   - **Definition:** TSS represents the total variability in the dependent variable (Y) and is the sum of the squared differences between each data point and the mean of the dependent variable. It quantifies the total dispersion in the data.

   - **Formula:** TSS = Σ(Yi - Ȳ)², where Yi is each individual data point, and Ȳ is the mean of the dependent variable Y.

   - **Interpretation:** TSS reflects the overall variability in the data before any regression model is applied. It is essentially a measure of how much the actual data points vary from the mean.

2. **RSS (Residual Sum of Squares):**
   - **Definition:** RSS quantifies the unexplained variation in the dependent variable that remains after fitting a regression model. It is the sum of the squared differences between each data point's actual value (Yi) and the predicted value (Ŷi) from the regression model.

   - **Formula:** RSS = Σ(Yi - Ŷi)², where Yi is the actual data point, and Ŷi is the predicted value from the regression model.

   - **Interpretation:** RSS measures the discrepancy between the observed data and the values predicted by the regression model. It represents the portion of the total variation in Y that cannot be explained by the independent variables.

3. **ESS (Explained Sum of Squares):**
   - **Definition:** ESS quantifies the portion of the total variability in the dependent variable that is explained by the regression model. It is the sum of the squared differences between the predicted values (Ŷi) and the mean of the dependent variable (Ȳ).

   - **Formula:** ESS = Σ(Ŷi - Ȳ)², where Ŷi is the predicted value from the regression model, and Ȳ is the mean of the dependent variable Y.

   - **Interpretation:** ESS measures how well the regression model accounts for the variation in the dependent variable. It represents the portion of the total variation in Y that can be explained by the independent variables.

The relationship between TSS, RSS, and ESS can be summarized using the following identity:

TSS = ESS + RSS

This identity states that the total variability in the dependent variable (TSS) can be decomposed into two components: the variability explained by the regression model (ESS) and the unexplained variability (RSS).

## Degree of Freedom

In the context of regression analysis, degrees of freedom (DF) refer to the number of values in the final calculation of various statistics (e.g., sums of squares, variances, F-statistics) that are free to vary. Degrees of freedom are an important concept when analyzing the variance components in a regression model, including Total Sum of Squares (TSS), Residual Sum of Squares (RSS), and Explained Sum of Squares (ESS). Let's explain the degrees of freedom for each of these components:

1. **Degrees of Freedom for TSS (Total Sum of Squares):**
   - **Definition:** TSS represents the total variability in the dependent variable Y. To calculate TSS, you need to determine the variation around the mean of Y.
   - **Formula:** TSS = Σ(Yi - Ȳ)², where Yi is each individual data point, and Ȳ is the mean of the dependent variable Y.
   - **Degrees of Freedom:** In the context of TSS, the degrees of freedom are equal to n - 1, where 'n' is the total number of data points. This is because one degree of freedom is used to estimate the mean of Y, leaving n - 1 degrees of freedom for the variability around the mean.

2. **Degrees of Freedom for RSS (Residual Sum of Squares):**
   - **Definition:** RSS quantifies the unexplained variation in Y, which is essentially the difference between the actual data points and the values predicted by the regression model.
   - **Formula:** RSS = Σ(Yi - Ŷi)², where Yi is the actual data point, and Ŷi is the predicted value from the regression model.
   - **Degrees of Freedom:** The degrees of freedom for RSS are equal to (n - k - 1), where 'k' is the number of independent variables (regressors) in the model. This accounts for the degrees of freedom used by the estimated model parameters (coefficients) and the 1 degree of freedom reserved for estimating the mean (intercept) of Y.

3. **Degrees of Freedom for ESS (Explained Sum of Squares):**
   - **Definition:** ESS quantifies the variability in Y that can be explained by the regression model, which represents the difference between the predicted values and the mean of Y.
   - **Formula:** ESS = Σ(Ŷi - Ȳ)², where Ŷi is the predicted value from the regression model, and Ȳ is the mean of the dependent variable Y.
   - **Degrees of Freedom:** The degrees of freedom for ESS are also equal to k, the number of independent variables in the model. This accounts for the degrees of freedom used by the estimated model parameters.

In summary, degrees of freedom are essential in regression analysis because they help in quantifying the number of values that are free to vary when calculating TSS, RSS, and ESS. Properly understanding and accounting for degrees of freedom are crucial when computing statistics, performing hypothesis tests, and interpreting results in regression analysis.

## F-statistic

The F-statistic, or F-test, is a statistical measure used in regression analysis to assess the overall significance and validity of a linear regression model. It helps answer the question of whether the model, which includes multiple independent variables, is providing a statistically significant improvement in explaining the variability in the dependent variable compared to a simpler model with no independent variables. Here's a detailed explanation of the F-statistic in regression analysis:

**1. Purpose of the F-Statistic:**

The F-statistic serves two primary purposes in regression analysis:

   a. **Testing Overall Significance:** It is used to test the overall significance of the regression model. In other words, it helps determine whether at least one of the independent variables in the model has a statistically significant relationship with the dependent variable.

   b. **Model Comparison:** It is used to compare the fit of a full model (with independent variables) to a reduced or simpler model (with no independent variables) to see if adding the independent variables results in a significant improvement in explaining the dependent variable.

**2. Formulation of the F-Statistic:**

The F-statistic is calculated by comparing the variance explained by the model (explained sum of squares, ESS) to the variance left unexplained by the model (residual sum of squares, RSS). The formula for the F-statistic in the context of regression is as follows:

$$
F = \frac{\text{ESS} / k}{\text{RSS} / (n - k - 1)}
$$

- ESS (Explained Sum of Squares) represents the variation explained by the regression model.
- RSS (Residual Sum of Squares) represents the unexplained variation or error in the model.
- k is the number of independent variables in the model.
- n is the total number of observations or data points.

**3. Hypotheses and Testing:**

The F-statistic is used to test a null hypothesis (H0) and an alternative hypothesis (Ha):

- Null Hypothesis (H0): The null hypothesis states that adding independent variables to the model does not significantly improve the fit of the model. Mathematically, it suggests that all coefficients (parameters) for the independent variables are equal to zero.

- Alternative Hypothesis (Ha): The alternative hypothesis suggests that at least one of the independent variables in the model is statistically significant, meaning that the coefficients for at least one of the variables are not equal to zero.

**4. Interpretation:**

The F-statistic produces a test statistic, which follows an F-distribution. If the calculated F-statistic is larger than the critical value from the F-distribution at a specified significance level (e.g., 0.05 or 0.01), then you reject the null hypothesis (H0) in favor of the alternative hypothesis (Ha). In other words, you conclude that at least one of the independent variables is statistically significant in explaining the variation in the dependent variable.

**5. Practical Use:**

The F-statistic is commonly used in multiple regression analysis, where there are multiple independent variables. It helps researchers and analysts determine whether the inclusion of these variables in the model is justified and whether the model as a whole provides a meaningful improvement in explaining the dependent variable.

In summary, the F-statistic is a critical tool in regression analysis that assesses the overall significance of a regression model and its ability to explain the variation in the dependent variable. It helps analysts make informed decisions about the inclusion of independent variables in the model.

## T-statistic

The T-statistic, or T-test, is a statistical measure used in regression analysis to assess the individual significance of each coefficient (parameter) associated with an independent variable in a linear regression model. It helps determine whether a specific independent variable has a statistically significant effect on the dependent variable. Here's a detailed explanation of the T-statistic in regression analysis:

**1. Purpose of the T-Statistic:**

The T-statistic is used to test the significance of individual coefficients in a linear regression model. It helps answer the question of whether each independent variable makes a statistically significant contribution to explaining the variation in the dependent variable.

**2. Formulation of the T-Statistic:**

The T-statistic for testing the significance of an individual coefficient is calculated using the following formula:

$$
T = \frac{\hat{\beta} - 0}{SE(\hat{\beta})}
$$

$$
SE(\hat{\beta}) = \sqrt{\frac{\sigma^2}{\sum{(x_i - \bar{x})^2}}}
$$

- T is the T-statistic.
- βˆ (beta hat) is the estimated coefficient for the independent variable.
- 0 represents the null hypothesis value for the coefficient (usually zero, indicating no effect).
- SE(βˆ) is the standard error of the coefficient estimate.

**3. Hypotheses and Testing:**

The T-statistic is used to test a null hypothesis (H0) and an alternative hypothesis (Ha):

- Null Hypothesis (H0): The null hypothesis for an individual coefficient test states that the coefficient is equal to zero, indicating that the corresponding independent variable has no effect on the dependent variable.

- Alternative Hypothesis (Ha): The alternative hypothesis suggests that the coefficient is not equal to zero, indicating that the corresponding independent variable has a statistically significant effect on the dependent variable.

**4. Interpretation:**

The T-statistic produces a test statistic that follows a T-distribution. To determine whether a coefficient is statistically significant, you compare the calculated T-statistic to the critical values from the T-distribution at a specified significance level (e.g., 0.05 or 0.01). If the T-statistic is larger in absolute value than the critical value, you reject the null hypothesis in favor of the alternative hypothesis, indicating that the coefficient is statistically significant.

**5. Practical Use:**

- In regression analysis, T-statistics are typically computed for each independent variable's coefficient. Analysts use these statistics to assess the significance of each variable's contribution to the model.

- T-statistics are often accompanied by their associated p-values. A low p-value (typically less than the chosen significance level) indicates statistical significance, while a high p-value suggests that the variable may not be statistically significant.

- Analysts use the results of T-tests to make informed decisions about including or excluding variables from the model and to understand which variables are the most relevant in explaining the dependent variable.

**6. Degrees of Freedom:**

The degrees of freedom for the T-test are typically determined by the number of observations (n) minus the number of coefficients being tested (k). The degrees of freedom are used to look up the critical values from the T-distribution table.

In summary, the T-statistic is a key tool in regression analysis used to test the significance of individual coefficients associated with independent variables in the model. It helps identify which variables have a meaningful impact on the dependent variable and guides decisions about model selection and variable inclusion.

## Regression Results

```html
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.530
Model:                            OLS   Adj. R-squared:                  0.373
Method:                 Least Squares   F-statistic:                     3.380
Date:                Sat, 21 Oct 2023   Prob (F-statistic):              0.163
Time:                        06:43:16   Log-Likelihood:                -5.9769
No. Observations:                   5   AIC:                             15.95
Df Residuals:                       3   BIC:                             15.17
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const          1.6579      1.253      1.323      0.278      -2.331       5.646
X              0.4868      0.265      1.839      0.163      -0.356       1.330
==============================================================================
Omnibus:                          nan   Durbin-Watson:                   3.482
Prob(Omnibus):                    nan   Jarque-Bera (JB):                0.534
Skew:                          -0.047   Prob(JB):                        0.766
Kurtosis:                       1.401   Cond. No.                         13.3
==============================================================================
```

Certainly, let's break down the content of a typical summary from an Ordinary Least Squares (OLS) regression analysis:

1. **Dependent Variable (Dep. Variable):**
   - This section specifies the dependent variable for the regression analysis. In your example, it is labeled as "y." The regression aims to explain and predict the variation in this dependent variable using the independent variable(s).

2. **R-squared (R-squared):**
   - R-squared (R²) is a measure of how well the independent variable(s) explain the variation in the dependent variable. It ranges from 0 to 1, where 1 indicates a perfect fit. In your example, R-squared is 0.530, suggesting that the model explains 53% of the variability in the dependent variable.

3. **Model:**
   - This section indicates the type of regression model used. "OLS" stands for Ordinary Least Squares, which is a common method for linear regression.

4. **Adj. R-squared (Adj. R-squared):**
   - Adjusted R-squared is a modified version of R-squared that takes into account the number of independent variables in the model. It provides a better indication of model fit when there are multiple predictors. In your example, the adjusted R-squared is 0.373.

5. **Method:**
   - "Least Squares" refers to the method used for parameter estimation in linear regression. It minimizes the sum of squared residuals to find the best-fitting model.

6. **F-statistic (F-statistic):**
   - The F-statistic tests the overall significance of the model. It assesses whether the model, with its independent variables, provides a significant improvement in explaining the dependent variable compared to a null model with no independent variables. In your example, the F-statistic is 3.380.

7. **Date and Time:**
   - These lines indicate the date and time when the regression analysis was conducted.

8. **Prob (F-statistic):**
   - This is the p-value associated with the F-statistic. A low p-value (typically less than 0.05) suggests that the model as a whole is statistically significant.

9. **Log-Likelihood:**
   - The log-likelihood is a measure of how well the model predicts the observed data. In your example, the log-likelihood is -5.9769.

10. **No. Observations:**
    - This line specifies the number of data points or observations used in the analysis. In your example, there are five observations.

11. **AIC and BIC (Akaike Information Criterion and Bayesian Information Criterion):**
    - AIC and BIC are information criteria used for model selection. Lower values are preferred, indicating a better fit. AIC and BIC consider the trade-off between model complexity and goodness of fit.

12. **Df Residuals and Df Model (Degrees of Freedom):**
    - "Df Residuals" is the degrees of freedom associated with the residuals (error terms), and "Df Model" is the degrees of freedom associated with the model parameters (coefficients). Degrees of freedom are important for hypothesis testing and model evaluation.

13. **Covariance Type:**
    - Specifies the type of covariance used in the analysis. In your example, "nonrobust" indicates that standard assumptions about residuals' independence and constant variance are applied.

14. **Coefficients and Their Statistics:**
    - The section below the line of hyphens provides detailed information about each coefficient (parameter) in the model, including the intercept (const) and the independent variable (X). This information includes the coefficient value (coef), standard error (std err), t-statistic (t), p-value (P>|t|), and a confidence interval (95% confidence interval: [0.025, 0.975]) for each coefficient.

15. **Omnibus, Prob(Omnibus), Skew, Kurtosis:**
    - These statistics provide additional information about the model's goodness of fit. "Omnibus" is a test for normality. "Skew" measures the skewness of the residuals, and "Kurtosis" measures their kurtosis (tailedness). "Prob(Omnibus)" and "Prob(JB)" are associated p-values.

16. **Durbin-Watson:**
    - The Durbin-Watson statistic tests for autocorrelation in the residuals. Values close to 2 suggest no significant autocorrelation.

17. **Jarque-Bera (JB) and Prob(JB):**
    - The Jarque-Bera test is another test for normality, and "Prob(JB)" is the associated p-value.

18. **Cond. No. (Condition Number):**
    - The condition number measures multicollinearity among the independent variables. A high condition number may indicate collinearity.

This summary provides a wealth of information to assess the quality of the regression model and the significance of the coefficients.

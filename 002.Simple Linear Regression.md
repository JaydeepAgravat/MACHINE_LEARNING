# Simple Linear Regression

- Linear regression is a fundamental statistical and machine learning technique used to model the relationship between a dependent variable and one or more independent variables.
- Simple Linear Regression specifically deals with the scenario where there is only one independent variable.
- It aims to find the best-fitting linear relationship between the independent variable (usually denoted as "X") and the dependent variable ("Y").
- The goal is to predict the values of the dependent variable based on the values of the independent variable.

## Mathematical Representation

- The equation of a simple linear regression can be represented as:

$$\ Y = \beta_0 + \beta_1 \cdot X + \epsilon \$$

- (Y) represents the dependent variable.
- (X) represents the independent variable.
- (β0) is the intercept term, representing the value of (Y) when (X) is 0.
- (β1) is the slope coefficient, indicating how the change in (X) influences the change in (Y).
- (ϵ) represents the error term, accounting for the variability that is not explained by the model.

## Objective

- The objective of simple linear regression is to find the values of (β0) and (β1) that minimize the sum of squared differences between the observed values of (Y) and the values predicted by the linear equation.

## Closed-Form Solution

- The closed-form solution, also known as the Ordinary Least Squares (OLS) solution, directly calculates the optimal values of (β0) and (β1) using mathematical formulas.

$$\ \beta_1 = \frac{n(\sum xy) - (\sum x)(\sum y)}{n(\sum x^2) - (\sum x)^2} \$$

$$\ \beta_0 = \frac{(\sum y) - \beta_1(\sum x)}{n} \$$

- (n) is the number of data points
- (∑) represents summation
- (x) is the independent variable values
- (y) is the dependent variable values

## Non Closed-Form Solution (Gradient Descent)

- While the closed-form solution works well for small datasets, it becomes computationally expensive for larger datasets.
- In such cases, gradient descent is often used to iteratively adjust the (β0) and (β1) coefficients.
- Gradient descent finds the optimal coefficients by minimizing the cost function, which is typically the mean squared error (MSE).

## Training the Model

- Training the model in the closed-form solution of simple linear regression involves finding the best-fitting line that minimizes the sum of squared differences between actual and predicted values.
- This is achieved by calculating the optimal coefficients (β0 and β1) using mathematical formulas based on the dataset's means and deviations.
- Once obtained, these coefficients define the linear equation.
- The model's performance is then evaluated using metrics like MSE, RMSE, and R2 by comparing predicted and actual values.
- This method works well for smaller datasets but can be less efficient for larger ones, where gradient descent might be preferred.

## Assumptions

1. **Linearity**: The relationship between (X) and (Y) is assumed to be linear.
2. **Independence**: Observations are independent of each other.
3. **Homoscedasticity**: The variance of the residuals is constant across all levels of \( X \).
4. **Normality**: Residuals are normally distributed.

## Use Cases

- **Economics**: Modeling the relationship between variables like income and spending.
- **Finance**: Predicting stock prices based on historical trends.
- **Medicine**: Relating dosage and treatment effectiveness.
- **Marketing**: Analyzing how advertising spend influences sales.

## Conclusion

- Simple linear regression is a foundational technique in machine learning and data science.
- It provides a way to understand and quantify the relationship between two variables through a linear equation.
- By fitting a line to the data, this method enables prediction and analysis, making it an essential tool in many real-world scenarios.

## Regression Metrics

- These regression metrics provide valuable insights into the performance and accuracy of your regression model.
- When interpreting these metrics, it's important to consider the context of your problem, the scale of the target variable, and the impact of outliers on the chosen metric.

### 1. **Mean Squared Error (MSE)**

- The Mean Squared Error is a widely used metric that measures the average squared difference between the predicted values and the actual values.
- It gives more weight to larger errors, making it sensitive to outliers.

    $$\ \text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 \$$

- (n) is the number of data points.
- (y_i) represents the actual target value.
- (y(hat)_i) represents the predicted value.

    **Advantages:** MSE gives higher weight to large errors due to squaring, which can be useful when large errors are considered more significant. It's a continuous, differentiable, and convex metric.

    **Disadvantages:** Because of squaring, outliers have a larger impact on MSE, potentially skewing the evaluation.

    **Use Case:** MSE is commonly used in regression tasks where predicting the magnitude of errors is important.

### 2. **Root Mean Squared Error (RMSE)**

- The Root Mean Squared Error is the square root of the MSE, providing a measure of the average error in the same unit as the target variable.
- RMSE is more interpretable than MSE and penalizes large errors more heavily.

    $$\ \text{RMSE} = \sqrt{\text{MSE}} \$$

    **Advantages:** RMSE is in the same unit as the target variable, making it more interpretable. It also penalizes large errors like MSE.

    **Disadvantages:** Similar to MSE, RMSE is sensitive to outliers and can be skewed by them.

    **Use Case:** RMSE is often used in regression tasks to measure the average magnitude of errors.

### 3. **Mean Absolute Error (MAE)**

- The Mean Absolute Error calculates the average absolute differences between the predicted values and the actual values.
- Like RMSE, it measures prediction accuracy, but it's less sensitive to outliers since it doesn't involve squaring the errors.

    $$\ \text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i| \$$

    **Advantages:** MAE is less sensitive to outliers compared to MSE and RMSE, as it doesn't involve squaring. It's more robust in the presence of extreme values.

    **Disadvantages:** It doesn't give more weight to larger errors, which might not align with certain applications.

    **Use Case:** MAE is used when you want a metric that provides a more balanced view of errors and doesn't heavily penalize outliers.

### 4. **Coefficient of Determination (R^2)**

- \( R^2 \) is a crucial metric that quantifies the proportion of the variance in the dependent variable that is explained by the independent variables in the model.
- It ranges from 0 to 1, where 0 indicates that the model doesn't explain any variance, and 1 indicates a perfect fit.

$$\ R^2 = 1 - \frac{SSR}{SST} \$$

- \( SSR \) is the sum of squared residuals (also known as the residual sum of squares).
- \( SST \) is the total sum of squares.

    **Advantages:** \(R^2\) gives an indication of how well the model's predictions match the variance of the actual data. It ranges from 0 to 1, where higher values indicate better fit.

    **Disadvantages:** \(R^2\) can still be high even if the model is overfitting or if the relationship between predictors and target is nonlinear.

    **Use Case:** \(R^2\) is a widely used metric to assess the goodness-of-fit of a regression model.

### 5. **Adjusted (R^2)**

- Adjusted \( R^2 \) is an extension of \( R^2 \) that accounts for the number of predictors in the model.
- It adjusts \( R^2 \) based on the complexity of the model and provides a better measure of the model's goodness of fit when comparing models with different numbers of predictors.

    $$\ \text{Adjusted } R^2 = 1 - \frac{(1 - R^2)(n - 1)}{n - p - 1} \$$

- (p) is the number of predictors in the model.

    **Advantages:** Adjusted \(R^2\) accounts for the number of predictors in the model, which helps prevent overfitting. It penalizes the addition of unnecessary predictors.

    **Disadvantages:** It may not be as intuitive to interpret as the standard \(R^2\) and doesn't always improve when adding predictors.

    **Use Case:** Adjusted \(R^2\) is helpful when you have multiple predictors and you want to evaluate the model's fit while considering model complexity.

### 6. **Residual Sum of Squares (RSS)**

- The Residual Sum of Squares is the sum of the squared differences between the actual target values and the predicted values.
- It represents the unexplained variance in the dependent variable.

    $$\ \text{RSS} = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 \$$

    **Advantages:** RSS measures the total amount of unexplained variation in the data, giving insight into how well the model captures the underlying patterns.

    **Disadvantages:** RSS on its own doesn't provide a normalized measure of fit, so it might be challenging to compare across different datasets.

    **Use Case:** RSS is often used in model evaluation and optimization, providing a quantitative measure of how well the model fits the data.
  
## Derivation of the Closed-Form Solution for Simple Linear Regression


$$\ \text{minimize} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 \$$

$$\ \text{minimize} \sum_{i=1}^{n} (y_i - (\beta_0 + \beta_1 x_i))^2 \$$

$$\ \frac{\partial}{\partial \beta_0} \sum_{i=1}^{n} (y_i - (\beta_0 + \beta_1 x_i))^2 = 0 \$$

$$\ \frac{\partial}{\partial \beta_1} \sum_{i=1}^{n} (y_i - (\beta_0 + \beta_1 x_i))^2 = 0 \$$

$$\ \beta_1 = \frac{n(\sum xy) - (\sum x)(\sum y)}{n(\sum x^2) - (\sum x)^2} \$$
$$\ \beta_0 = \frac{(\sum y) - \beta_1(\sum x)}{n} \$$

Differentiating with respect to (β0):

$$\ \frac{\partial}{\partial \beta_0} \sum_{i=1}^{n} (y_i - (\beta_0 + \beta_1 x_i))^2 \$$

Using the chain rule of differentiation, we get:

$$\ -2 \sum_{i=1}^{n} (y_i - (\beta_0 + \beta_1 x_i)) \$$

Setting this derivative to zero:

$$\ -2 \sum_{i=1}^{n} (y_i - (\beta_0 + \beta_1 x_i)) = 0 \$$

$$\ \sum_{i=1}^{n} y_i - n \beta_0 - \beta_1 \sum_{i=1}^{n} x_i = 0 \$$
$$\ \sum_{i=1}^{n} y_i = n \beta_0 + \beta_1 \sum_{i=1}^{n} x_i \$$
$$\ \beta_0 = \frac{(\sum y) - \beta_1(\sum x)}{n} \$$

Differentiating with respect to (β1):

$$\ \frac{\partial}{\partial \beta_1} \sum_{i=1}^{n} (y_i - (\beta_0 + \beta_1 x_i))^2 \$$

Using the chain rule again, we get:
$$\ -2 \sum_{i=1}^{n} x_i (y_i - (\beta_0 + \beta_1 x_i)) \$$

Setting this derivative to zero:

$$\ -2 \sum_{i=1}^{n} x_i (y_i - (\beta_0 + \beta_1 x_i)) = 0 \$$

$$\ \sum_{i=1}^{n} x_i y_i - \beta_0 \sum_{i=1}^{n} x_i - \beta_1 \sum_{i=1}^{n} x_i^2 = 0 \$$

$$\ \sum_{i=1}^{n} x_i y_i - \frac{(\sum y) - \beta_1(\sum x)}{n} \sum_{i=1}^{n} x_i - \beta_1 \sum_{i=1}^{n} x_i^2 = 0 \$$

$$\ \sum_{i=1}^{n} x_i y_i - \frac{(\sum y)(\sum x)}{n} + \beta_1 \frac{(\sum x)^2}{n} - \beta_1 \sum_{i=1}^{n} x_i^2 = 0 \$$

$$\ \beta_1 \frac{n(\sum xy) - (\sum x)(\sum y)}{n} = \frac{(\sum y)(\sum x)}{n} - \sum_{i=1}^{n} x_i y_i \$$

$$\ \beta_1 = \frac{n(\sum xy) - (\sum x)(\sum y)}{n(\sum x^2) - (\sum x)^2} \$$
